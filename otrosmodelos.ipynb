{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61f0b64e-b022-4555-8778-debdd3deca32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path \n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc6b2a12-9de0-48bb-b541-e9a3b03104c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subcarpetas encontradas: 29 ⇒ ['R', 'U', 'I', 'N', 'G', 'Z', 'T', 'S', 'A', 'F']...\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "DATA_DIR = Path(\"asl_alphabet_train/asl_alphabet_train\")\n",
    "subdirs = [p.name for p in DATA_DIR.iterdir() if p.is_dir()]\n",
    "print(f\"Subcarpetas encontradas: {len(subdirs)} ⇒ {subdirs[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94d56fb9-2cbe-4803-b31a-3a41c8a91bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(img_size=64, limit_per_class=None):\n",
    "    X, y = [], []\n",
    "    for idx, category in enumerate(CATEGORIES):\n",
    "        folder = Path(DATA_DIR) / category\n",
    "        imgs = sorted(folder.iterdir())\n",
    "        if limit_per_class:\n",
    "            imgs = imgs[:limit_per_class]          # para debug rápido\n",
    "        for img_path in imgs:\n",
    "            img = cv2.imread(str(img_path))\n",
    "            if img is None:                # <-- imagen corrupta o mal leída\n",
    "                print(f\"[WARN] Saltando {img_path}\")\n",
    "                continue\n",
    "            img = cv2.resize(img, (img_size, img_size), cv2.INTER_AREA)\n",
    "            X.append(img)\n",
    "            y.append(idx)\n",
    "    X = np.asarray(X, dtype=\"float32\") / 255.0\n",
    "    y = to_categorical(y, num_classes=len(CATEGORIES))\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1845230-70ba-4a0b-9710-4e9d9037ac21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 69600 images belonging to 29 classes.\n",
      "Found 17400 images belonging to 29 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "IMG_SIZE = 64\n",
    "BATCH = 32\n",
    "SEED = 42\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,  # <<<<<< ESTA LÍNEA ES CLAVE\n",
    "    rotation_range=30,\n",
    "    zoom_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    horizontal_flip=True,  # Si tiene sentido\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    DATA_DIR,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    DATA_DIR,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    seed=SEED\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7feb07a0-f3d2-44db-86d5-74ba231fa754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 Entrenando baseline ...\n",
      "Epoch 1/10\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 39ms/step - accuracy: 0.1441 - loss: 2.9596 - val_accuracy: 0.2500 - val_loss: 2.3899\n",
      "Epoch 2/10\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 38ms/step - accuracy: 0.4183 - loss: 1.8408 - val_accuracy: 0.3855 - val_loss: 1.8900\n",
      "Epoch 3/10\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 39ms/step - accuracy: 0.5292 - loss: 1.4471 - val_accuracy: 0.4806 - val_loss: 1.6049\n",
      "Epoch 4/10\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 39ms/step - accuracy: 0.5937 - loss: 1.2177 - val_accuracy: 0.5233 - val_loss: 1.4140\n",
      "Epoch 5/10\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 38ms/step - accuracy: 0.6414 - loss: 1.0703 - val_accuracy: 0.5348 - val_loss: 1.3882\n",
      "Epoch 6/10\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 38ms/step - accuracy: 0.6755 - loss: 0.9727 - val_accuracy: 0.5589 - val_loss: 1.3195\n",
      "Epoch 7/10\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 39ms/step - accuracy: 0.7009 - loss: 0.8884 - val_accuracy: 0.5626 - val_loss: 1.3069\n",
      "Epoch 8/10\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 39ms/step - accuracy: 0.7255 - loss: 0.8093 - val_accuracy: 0.5997 - val_loss: 1.2316\n",
      "Epoch 9/10\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 38ms/step - accuracy: 0.7450 - loss: 0.7563 - val_accuracy: 0.6006 - val_loss: 1.2635\n",
      "Epoch 10/10\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 38ms/step - accuracy: 0.7575 - loss: 0.7103 - val_accuracy: 0.5941 - val_loss: 1.2495\n",
      "➡️  baseline: best val_accuracy = 0.6006\n",
      "\n",
      "🔧 Entrenando mobilenet ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nb/c0sg3gpn54s2yvl29bhbcx9h0000gn/T/ipykernel_10384/969221915.py:21: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base = MobileNetV2(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 36ms/step - accuracy: 0.4333 - loss: 1.9447 - val_accuracy: 0.4010 - val_loss: 2.1429\n",
      "Epoch 2/10\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 39ms/step - accuracy: 0.5994 - loss: 1.2886 - val_accuracy: 0.4353 - val_loss: 2.0579\n",
      "Epoch 3/10\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 78ms/step - accuracy: 0.6398 - loss: 1.1673 - val_accuracy: 0.4415 - val_loss: 2.0170\n",
      "Epoch 4/10\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 77ms/step - accuracy: 0.6541 - loss: 1.1021 - val_accuracy: 0.4458 - val_loss: 2.0777\n",
      "Epoch 5/10\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 78ms/step - accuracy: 0.6725 - loss: 1.0451 - val_accuracy: 0.4644 - val_loss: 2.0213\n",
      "Epoch 6/10\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 94ms/step - accuracy: 0.6848 - loss: 1.0087 - val_accuracy: 0.4600 - val_loss: 2.0562\n",
      "Epoch 7/10\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 110ms/step - accuracy: 0.6895 - loss: 0.9892 - val_accuracy: 0.4643 - val_loss: 2.0467\n",
      "Epoch 8/10\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 97ms/step - accuracy: 0.6965 - loss: 0.9656 - val_accuracy: 0.4776 - val_loss: 1.9810\n",
      "Epoch 9/10\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 89ms/step - accuracy: 0.7062 - loss: 0.9441 - val_accuracy: 0.4705 - val_loss: 2.0513\n",
      "Epoch 10/10\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 86ms/step - accuracy: 0.7136 - loss: 0.9159 - val_accuracy: 0.4614 - val_loss: 2.1167\n",
      "➡️  mobilenet: best val_accuracy = 0.4776\n",
      "\n",
      "🔧 Entrenando efficientnet ...\n",
      "Epoch 1/10\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 121ms/step - accuracy: 0.0339 - loss: 3.3869 - val_accuracy: 0.0345 - val_loss: 3.3674\n",
      "Epoch 2/10\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 121ms/step - accuracy: 0.0347 - loss: 3.3677 - val_accuracy: 0.0345 - val_loss: 3.3674\n",
      "Epoch 3/10\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 121ms/step - accuracy: 0.0342 - loss: 3.3675 - val_accuracy: 0.0345 - val_loss: 3.3673\n",
      "Epoch 4/10\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 119ms/step - accuracy: 0.0354 - loss: 3.3675 - val_accuracy: 0.0345 - val_loss: 3.3674\n",
      "Epoch 5/10\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 124ms/step - accuracy: 0.0344 - loss: 3.3675 - val_accuracy: 0.0345 - val_loss: 3.3674\n",
      "Epoch 6/10\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 115ms/step - accuracy: 0.0325 - loss: 3.3676 - val_accuracy: 0.0345 - val_loss: 3.3674\n",
      "Epoch 7/10\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 119ms/step - accuracy: 0.0327 - loss: 3.3676 - val_accuracy: 0.0345 - val_loss: 3.3674\n",
      "Epoch 8/10\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 117ms/step - accuracy: 0.0332 - loss: 3.3676 - val_accuracy: 0.0345 - val_loss: 3.3674\n",
      "Epoch 9/10\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 120ms/step - accuracy: 0.0340 - loss: 3.3675 - val_accuracy: 0.0345 - val_loss: 3.3674\n",
      "Epoch 10/10\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 116ms/step - accuracy: 0.0341 - loss: 3.3676 - val_accuracy: 0.0345 - val_loss: 3.3674\n",
      "➡️  efficientnet: best val_accuracy = 0.0345\n",
      "\n",
      "=== RESUMEN ===\n",
      "baseline     : 0.6006\n",
      "mobilenet    : 0.4776\n",
      "efficientnet : 0.0345\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.applications import MobileNetV2, EfficientNetB0\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "IMG_SHAPE = (64, 64, 3)\n",
    "N_CLASSES = 29\n",
    "LR = 1e-3\n",
    "EPOCHS = 10      # pon 10‑15 para el barrido inicial\n",
    "\n",
    "def build_baseline():\n",
    "    x_in = Input(shape=IMG_SHAPE)\n",
    "    x = Conv2D(32,(3,3),activation='relu')(x_in); x = MaxPooling2D()(x)\n",
    "    x = Conv2D(64,(3,3),activation='relu')(x); x = MaxPooling2D()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128,activation='relu')(x); x = Dropout(0.3)(x)\n",
    "    out = Dense(N_CLASSES, activation='softmax')(x)\n",
    "    return Model(x_in, out)\n",
    "\n",
    "def build_mobilenet():\n",
    "    base = MobileNetV2(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\n",
    "    base.trainable = False          # primer round: solo cabeza\n",
    "    x = Flatten()(base.output)\n",
    "    x = Dense(256,activation='relu')(x)\n",
    "    out = Dense(N_CLASSES, activation='softmax')(x)\n",
    "    return Model(base.input, out)\n",
    "\n",
    "def build_efficientnet():\n",
    "    base = EfficientNetB0(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\n",
    "    base.trainable = False\n",
    "    x = Flatten()(base.output)\n",
    "    x = Dense(256,activation='relu')(x)\n",
    "    out = Dense(N_CLASSES, activation='softmax')(x)\n",
    "    return Model(base.input, out)\n",
    "\n",
    "models_to_try = {\n",
    "    \"baseline\": build_baseline,\n",
    "    \"mobilenet\": build_mobilenet,\n",
    "    \"efficientnet\": build_efficientnet\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, builder in models_to_try.items():\n",
    "    print(f\"\\n🔧 Entrenando {name} ...\")\n",
    "    model = builder()\n",
    "    model.compile(optimizer=Adam(LR), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    h = model.fit(train_gen,\n",
    "                  epochs=EPOCHS,\n",
    "                  validation_data=val_gen,\n",
    "                  callbacks=[],\n",
    "                  verbose=1)\n",
    "    best_val = max(h.history['val_accuracy'])\n",
    "    results[name] = best_val\n",
    "    model.save(f\"{name}.keras\")\n",
    "    print(f\"➡️  {name}: best val_accuracy = {best_val:.4f}\")\n",
    "\n",
    "print(\"\\n=== RESUMEN ===\")\n",
    "for n,a in results.items():\n",
    "    print(f\"{n:<12} : {a:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab37cc09-686d-4b92-b185-75fe1b9429d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "IMG_SHAPE = (64, 64, 3)\n",
    "N_CLASSES = 29  # Cambia si tienes más/menos letras\n",
    "\n",
    "# Modelo base preentrenado\n",
    "base_model = MobileNetV2(\n",
    "    input_shape=IMG_SHAPE,\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "base_model.trainable = False  # Congelar capas preentrenadas\n",
    "\n",
    "# Añadir nueva cabeza de clasificación\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "output = Dense(N_CLASSES, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# Compilar\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Entrenar\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea5ba9a-7b98-4820-927a-bff0ea22e0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desbloquear parte de MobileNet\n",
    "base_model.trainable = True\n",
    "\n",
    "# Opción: solo afinar las últimas N capas\n",
    "for layer in base_model.layers[:-30]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompilar con menor LR\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(factor=0.2, patience=2)\n",
    "]\n",
    "\n",
    "# Fine-tune\n",
    "history_ft = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=15,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1989ae8e-02e7-4f24-b8fd-8332ef957611",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(test_gen)\n",
    "print(f\"🔍 Accuracy final en test: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b806cf-9684-4372-b7dc-08b971d5e91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "model = load_model(\"efficientnet.keras\")\n",
    "\n",
    "# ⚠️ Descongelar las últimas N capas del backbone\n",
    "UNFREEZE = 30\n",
    "for layer in model.layers[-UNFREEZE:]:\n",
    "    if not isinstance(layer, Dense):   # evita re‑compilar las densas\n",
    "        layer.trainable = True\n",
    "\n",
    "model.compile(optimizer=Adam(1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(factor=0.2, patience=3, verbose=1),\n",
    "    EarlyStopping(patience=6, restore_best_weights=True, verbose=1)\n",
    "]\n",
    "\n",
    "history_ft = model.fit(\n",
    "    train_gen,\n",
    "    epochs=20,\n",
    "    validation_data=val_gen,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "model.save(\"asl_best_finetuned.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e57f697-ad4d-4d3a-954d-ea40709eb730",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = ImageDataGenerator(rescale=1/255).flow_from_directory(\n",
    "    \"asl_alphabet_test\",\n",
    "    target_size=(64,64),\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "loss, acc = model.evaluate(test_gen)\n",
    "print(f\"📊 Accuracy test: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392cccde-c1ff-4dfb-9875-6b8322e38700",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='Entrenamiento')\n",
    "plt.plot(history.history['val_accuracy'], label='Validación')\n",
    "plt.title(\"Precisión del modelo\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Precisión\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5e0918-3a23-4701-93fb-b9effe53ce5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eb4263-0110-4c37-8e2a-9e32c14ac571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "y_true = test_gen.classes\n",
    "y_pred = model.predict(test_gen, verbose=0).argmax(axis=1)\n",
    "print(classification_report(y_true, y_pred, target_names=test_gen.class_indices.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0118103a-b20a-4f85-a00c-c169862e41c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(\"efficientnet.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd99d6e-c4ed-45be-b119-5be2da978b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "test_gen = test_datagen.flow_from_directory(\n",
    "    \"asl_alphabet_test\",\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "loss, acc = model.evaluate(test_gen)\n",
    "print(f\"📊 Accuracy en test: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f0d3f6-a5bb-4b1f-9d02-a326e1c53bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_true = test_gen.classes\n",
    "y_pred = model.predict(test_gen, verbose=1).argmax(axis=1)\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=test_gen.class_indices.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee7450c-e456-4dd6-9fbe-f5ad3c027b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=False, cmap='Blues')\n",
    "plt.title(\"Matriz de Confusión\")\n",
    "plt.xlabel(\"Predicción\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73464b2-252c-4ff5-95de-1db6d448c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# ===== Mostrar máximos =====\n",
    "best_epoch = int(np.argmax(history.history['val_accuracy']))\n",
    "best_val  = history.history['val_accuracy'][best_epoch]\n",
    "best_train = history.history['accuracy'][best_epoch]\n",
    "print(f\"Mejor época: {best_epoch+1}  |  \"\n",
    "      f\"accuracy train: {best_train:.4f}  |  accuracy val: {best_val:.4f}\")\n",
    "\n",
    "# ===== Graficar curva =====\n",
    "pd.DataFrame(history.history)[['accuracy','val_accuracy']].plot()\n",
    "plt.title(\"Evolución de accuracy\"); plt.xlabel(\"Época\"); plt.ylabel(\"Accuracy\"); plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c3f9c2-add8-4576-b394-b8914689b74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Graficar Accuracy ---\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Entrenamiento')\n",
    "plt.plot(history.history['val_accuracy'], label='Validación')\n",
    "plt.title(\"Precisión (Accuracy)\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# --- Graficar Loss ---\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Entrenamiento')\n",
    "plt.plot(history.history['val_loss'], label='Validación')\n",
    "plt.title(\"Pérdida (Loss)\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e320f6f4-5e46-4009-8891-64a03bb4dc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_ft.history['accuracy']  # o 'val_accuracy', etc.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
